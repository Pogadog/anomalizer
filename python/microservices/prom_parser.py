# prometheus expression parser implemented using funcparserlib. built to handle basic prometheus queried as
# generated by The Anomalizer such as these:
#   * anomalizer_active_threads
#   * anomalixer_active_threads{job="anomaliser"}
#   * rate(anomalizer_active_threads[5m])
#   * rate(anomalizer_active_threads{job="anomalizer-engine"}[5m])
#

from typing import List, Tuple, Union
from funcparserlib.lexer import make_tokenizer, TokenSpec, Token
from funcparserlib.parser import tok, Parser, many, maybe, forward_decl, finished
from dataclasses import dataclass
import pandas as pd
import numpy as np

@dataclass
class BinaryExpr:
    op: str
    left: "Expr"
    right: "Expr"

@dataclass
class FunctionalExpr:
    op: str
    args: "ArgsExpr"
    rate: "Rate"

@dataclass
class ArgsExpr:
    args: "Args"

@dataclass
class TaggedExpr:
    args: "tuple"

Expr = Union[BinaryExpr, FunctionalExpr, int, float]

def to_expr(args: Tuple[Expr, List[Tuple[str, Expr]]]) -> Expr:
    first, rest = args
    result = first
    for op, expr in rest:
        result = BinaryExpr(op, result, expr)
    return result

def to_tags(tags):
    return tags

MUL = {'s': 1, 'm': 60, 'h': 60*60}

def to_func(args):
    if args[2]:
        rate = int(args[2][1:-2])
        rate *= MUL[args[2][-2]]
    else:
        rate = 0
    return FunctionalExpr(args[0], args[1], rate)

def to_tagged(args):
    if args[1]:
        return TaggedExpr([args[0]] + [[(a[0], a[1], a[2]) for a in args[1]]])
    else:
        return args[0]

def to_args(args):
    result = ArgsExpr([args[0]] +  [a[0] for a in args[1:][0]])
    return result

def tokenize(s: str) -> List[Token]:
    specs = [
        TokenSpec("rate", r'\[\d+(s|m|h)\]'),
        TokenSpec('string', r'"[^\"]*"'),
        TokenSpec("float", r"\d+\.\d*([Ee][+\-]?\d+)*"),
        TokenSpec("int", r"\d+"),
        TokenSpec('whitespace', r'\s+'), TokenSpec('string', r'"[^\"]*"'),
        TokenSpec('name', r'[\w]+'), TokenSpec('op', r'(=~)|[(){}=,+\-*/]'),
    ]
    '''
    specs = [
        TokenSpec('sum', 'sum'), TokenSpec('rate', 'rate'), TokenSpec('whitespace', r'\s+'), TokenSpec('string', r'"[^\"]*"'),
        TokenSpec('time', r'\[\d+[smh]\]'), TokenSpec('name', r'[\w]+'), TokenSpec('op', r'[(){}=~,+\-*/]'),
        TokenSpec('tilde', r'~')
    ]
    '''
    tokenizer = make_tokenizer(specs)
    return [t for t in tokenizer(s) if t.type != 'whitespace']

def op(name: str) -> Parser[Token, str]:
    return tok("op", name)

def parse(string: str):
    return document.parse(tokenize(string))

expr = forward_decl()
args = expr + -op(',') + many(expr + maybe(-op(','))) >> to_args
func = maybe(tok('name')) + -op('(') + (args | expr) + maybe(tok('rate')) + -op(')') >> to_func
paren = -op('(') + expr + -op(')')
int_num = tok("int") >> int
float_num = tok("float") >> float
number = int_num | float_num

tags = -op('{') + many(tok('name') + (op('=~') | op('=')) + tok('string') + maybe(-op(','))) + -op('}') >> to_tags
subexpr = (func | paren | tok('name')) + maybe(tags) >> to_tagged | number
mul = subexpr + many((op('*') | op('/')) + subexpr) >> to_expr
sum = mul + many((op('+') | op('-')) + mul) >> to_expr
expr.define(sum)
document = expr + -finished

def _max(x):
    if isinstance(x, pd.DataFrame) and x.shape[1]>1:
        return x.max(axis=1)
    else:
        return float(x.max())

OPERATORS = {'+': pd.DataFrame.__add__, '-': pd.DataFrame.__sub__, '*': pd.DataFrame.__mul__, '/': pd.DataFrame.__truediv__}
FUNCTIONS = {'sum': lambda x,r: pd.DataFrame.sum(x, axis=1), 'max': lambda x,r: _max(x),
             'rate': lambda x,r: x.diff().clip(lower=0).fillna(0).rolling(max(1, r//60)).mean().fillna(0), None: lambda x,r: x}

def eval_tree(data: dict, expr: Expr):
    import numbers
    if isinstance(expr, numbers.Number):
        return expr
    if isinstance(expr, BinaryExpr):
        return OPERATORS[expr.op](eval_tree(data, expr.left), eval_tree(data, expr.right)).dropna(axis=1).fillna(0)
    elif isinstance(expr, FunctionalExpr):
        args = eval_tree(data, expr.args)
        return FUNCTIONS[expr.op](args, expr.rate)
    elif isinstance(expr, ArgsExpr):
        result = [eval_tree(data, e) for e in expr.args]
        df = pd.concat(result, axis=1)
        df.columns = expr.args
        return df
    elif isinstance(expr, TaggedExpr):
        df = eval_tree(data, expr.args[0])
        filter = expr.args[1]
        # set up exact and regex matches.
        exacts = {}
        regexs = {}
        for f in filter:
            if f[1]=='=':
                exacts[f[0]] = f[2].replace('"', '')
            else:
                regexs[f[0]] = f[2].replace('"', '')
        result = pd.DataFrame();
        # match exact and regex terms for each column.
        for col,values in df.iteritems():
            tags = col.split(',')
            tags = dict([(t.split('=')[0].strip(), t.split('=')[1].replace('"', '').strip()) for t in tags])
            found = True
            for e in exacts:
                if tags.get(e)!=exacts[e]:
                    found = False
                    break
            if found:
                for r in regexs:
                    if not re.match(tags.get(r, '$'), regexs[r]):
                        found = False
                        break
            if found:
                result[col] = values
        return result
    # get a raw expression from the data source.
    _type = data['types'].get(expr)
    if _type=='counter':
        expr += '_total'
    tagged = data['metrics'].get(expr, {})
    result = pd.DataFrame()
    try:
        for tag in tagged:
            tdf = pd.DataFrame(tagged[tag])
            df = tdf.iloc[:,1:]
            df.index = tdf.iloc[:,0]
            df.columns = [tag]
            result = pd.concat([result, df], axis=1)
        result = result.interpolate() # fix nulls caused by offsets in times between sources.
    except:
        import traceback
        traceback.print_exc()
    return result

'''
equ = op('=') + maybe(op('~'))
tag = tok('name') + equ + tok('string')
tags = -op('{') + many(tag + -maybe(op(','))) + -op('}')
metric = tok('name') + maybe(tags) + maybe(tok('time'))
rate = tok('rate') + -op('(') + metric + -op(')') | -maybe(op('(')) + metric + -maybe(op(')'))
'''

if __name__=='__main__':
    print(tokenize('metric_name "abc_def" [5m] "abc-def" job=~"foo"'))
    print(tokenize('sum(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / sum(node_memory_MemTotal_bytes)*100'))
    print(parse('metric_name'))
    print(parse('metric_name{job="foo"}'))
    print(parse('metric_name{job="foo", target=~"bar"}'))
    print(parse('rate(metric_name{job="foo"}[5m])'))
    print(parse('rate(metric_name)'))
    print(parse('rate(metric_name[5m])'))
    #print(sum.parse(tokenize('sum(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)')))
    #print(expr.parse(tokenize('(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)/abc')))

    parsed = parse('rate(anomalizer_active_threads{job="anomalizer-engine"}[5m])')
    print(parsed)

'''
from dataclasses import dataclass
from funcparserlib.util import pretty_tree
@dataclass
class BinaryExpr:
    op: str
    left: "Expr"
    right: "Expr"

Expr = Union[BinaryExpr, int, float]

def tokenize(s: str) -> List[Token]:
    specs = [
        TokenSpec("whitespace", r"\s+"),
        TokenSpec("float", r"[+\-]?\d+\.\d*([Ee][+\-]?\d+)*"),
        TokenSpec("int", r"[+\-]?\d+"),
        TokenSpec("op", r"(\*\*)|[+\-*/()]"),
    ]
    tokenizer = make_tokenizer(specs)
    return [t for t in tokenizer(s) if t.type != "whitespace"]


def parse(tokens: List[Token]) -> Expr:
    int_num = tok("int") >> int
    float_num = tok("float") >> float
    number = int_num | float_num

    expr: Parser[Token, Expr] = forward_decl()
    parenthesized = -op("(") + expr + -op(")")
    primary = number | parenthesized
    power = primary + many(op("**") + primary) >> to_expr
    term = power + many((op("*") | op("/")) + power) >> to_expr
    sum = term + many((op("+") | op("-")) + term) >> to_expr
    expr.define(sum)

    document = expr + -finished

    return document.parse(tokens)

def op(name: str) -> Parser[Token, str]:
    return tok("op", name)

def to_expr(args: Tuple[Expr, List[Tuple[str, Expr]]]) -> Expr:
    first, rest = args
    result = first
    for op, expr in rest:
        result = BinaryExpr(op, result, expr)
    return result

print(parse(tokenize("0")))
print(parse(tokenize("1 + 2 + 3")))

from funcparserlib.util import pretty_tree

def pretty_expr(expr: Expr) -> str:

    def kids(expr: Expr) -> List[Expr]:
        if isinstance(expr, BinaryExpr):
            return [expr.left, expr.right]
        else:
            return []

    def show(expr: Expr) -> str:
        if isinstance(expr, BinaryExpr):
            return f"BinaryExpr({expr.op!r})"
        else:
            return repr(expr)

    return pretty_tree(expr, kids, show)

print(pretty_expr(parse(tokenize("1 + 2 * 3"))))

'''